{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07ae778-7221-44fa-8aeb-45eab1d4000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from onnxutils.common import OnnxModel\n",
    "from onnxutils.onnx2torch import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e702a7db-9ce4-40ac-aebd-bd1ff374ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/ycgao/Workdir/dataset/unimodel_calibrate'\n",
    "model_path = 'unimodel.optimized.onnx'\n",
    "qmodel_path = 'unimodel.quantized.onnx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52abc669-4ad5-40b7-a753-79ec425795ae",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247cd420-86c1-4909-a87b-0d5700f782c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = OnnxModel.from_file(model_path)\n",
    "with onnx_model.session() as sess:\n",
    "    for node in onnx_model.proto().graph.node:\n",
    "        if node.name == '':\n",
    "            node.name = sess.unique_name()\n",
    "\n",
    "torch_model = convert(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fbfa1-f570-4d20-8b8a-5e73565d4afe",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013255ee-eb23-4881-b5a9-2f938ab25c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxutils.common import DatasetUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa1782b-08f0-4d46-8c16-ce81396c3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnimodelDataset:\n",
    "    fields = [\n",
    "        (\"imgs\", [10, 3, 576, 960], np.float32),\n",
    "        (\"fused_projection\", [1, 10, 4, 4], np.float32),\n",
    "        (\"pose\", [1, 4, 4], np.float32),\n",
    "        (\"prev_pose_inv\", [1, 4, 4], np.float32),\n",
    "        (\"extrinsics\", [1, 10, 4, 4], np.float32),\n",
    "        (\"norm_intrinsics\", [1, 10, 4, 4], np.float32),\n",
    "        (\"distortion_coeff\", [1, 10, 6], np.float32),\n",
    "        (\"prev_bev_feats\", [1, 48, 60, 77], np.float32),\n",
    "        (\"sdmap_encode\", [1, 9, 128, 160], np.float32),\n",
    "        (\"mpp\", [1, 3, 224, 384], np.float32),\n",
    "        (\"mpp_pose_state\", [1, 6], np.float32),\n",
    "        (\"mpp_valid\", [1, 1], np.float32),\n",
    "        (\"prev_feat_stride16\", [1, 48, 36, 60], np.float32),\n",
    "        (\"sdmap_mat\", [1, 4, 240, 400], np.float32),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.snippets = os.listdir(self.root_dir)\n",
    "\n",
    "    def load_item(self, path):\n",
    "        return {\n",
    "            f[0]: torch.from_numpy(np.fromfile(os.path.join(\n",
    "                path, f\"{f[0]}.bin\"), dtype=f[2]).reshape(f[1]))\n",
    "            for f in UnimodelDataset.fields\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.snippets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.load_item(os.path.join(self.root_dir, str(idx)))\n",
    "\n",
    "\n",
    "dataset = UnimodelDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79104cf-f906-4276-9733-02a5d7b0167c",
   "metadata": {},
   "source": [
    "# Quantize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a83f36-c811-49d3-baf5-753eac6be217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization.qconfig_mapping import QConfigMapping, get_default_qconfig_mapping\n",
    "from torch.ao.quantization.qconfig import QConfig\n",
    "from torch.ao.quantization.observer import ReuseInputObserver, NoopObserver, HistogramObserver, MinMaxObserver\n",
    "from torch.ao.quantization.fx.custom_config import PrepareCustomConfig, ConvertCustomConfig\n",
    "\n",
    "from torch.nn.intrinsic.modules.fused import ConvReLU2d\n",
    "\n",
    "from onnxutils.onnx2torch.scatter_nd import TorchScatterNd\n",
    "from onnxutils.onnx2torch.converter import normalize_module_name\n",
    "from onnxutils.onnx2torch.utils import OnnxMapping\n",
    "\n",
    "def fix_onnx_mapping(torch_model):\n",
    "    for m in torch_model.children():\n",
    "        fix_onnx_mapping(m)\n",
    "\n",
    "    if isinstance(torch_model, (ConvReLU2d, )):\n",
    "        conv = getattr(torch_model, '0')\n",
    "        relu = getattr(torch_model, '1')\n",
    "\n",
    "        if hasattr(conv, 'onnx_mapping') and hasattr(relu, 'onnx_mapping'):\n",
    "            torch_model.onnx_mapping = OnnxMapping(\n",
    "                inputs=conv.onnx_mapping.inputs,\n",
    "                outputs=relu.onnx_mapping.outputs\n",
    "            )\n",
    "\n",
    "def quantize_model(torch_model, qconfig_mapping, dataset, loss_fn=None):\n",
    "    prepare_custom_config = PrepareCustomConfig()\n",
    "    prepare_custom_config.set_non_traceable_module_classes([TorchScatterNd])\n",
    "    prepare_custom_config.set_preserved_attributes(['onnx_mapping'])\n",
    "\n",
    "    torch_model.eval().cpu()\n",
    "    model_prepared = prepare_fx(\n",
    "        torch_model,\n",
    "        qconfig_mapping,\n",
    "        dataset[0],\n",
    "        prepare_custom_config\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        DatasetUtils.transform(\n",
    "            dataset,\n",
    "            lambda item: tuple(item[x].cuda() for x in torch_model.onnx_mapping.inputs)\n",
    "        ),\n",
    "        batch_size=None\n",
    "    )\n",
    "    model_prepared.cuda()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            model_prepared(*data)\n",
    "    \n",
    "    model_prepared.cpu()\n",
    "    convert_custom_config = ConvertCustomConfig()\n",
    "    convert_custom_config.set_preserved_attributes(['onnx_mapping'])\n",
    "    model_converted = convert_fx(model_prepared, convert_custom_config=convert_custom_config)\n",
    "    fix_onnx_mapping(model_converted)\n",
    "    return model_converted\n",
    "\n",
    "def quantize_qat_model(torch_model, qconfig_mapping, dataset, optimizer=None, loss_fn=None):\n",
    "    prepare_custom_config = PrepareCustomConfig()\n",
    "    prepare_custom_config.set_non_traceable_module_classes([TorchScatterNd])\n",
    "    prepare_custom_config.set_preserved_attributes(['onnx_mapping'])\n",
    "\n",
    "    torch_model.train().cpu()\n",
    "    model_prepared = prepare_qat_fx(\n",
    "        torch_model,\n",
    "        qconfig_mapping,\n",
    "        dataset[0],\n",
    "        prepare_custom_config\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        DatasetUtils.transform(\n",
    "            dataset,\n",
    "            lambda item: tuple(item[x].cuda() for x in torch_model.onnx_mapping.inputs)\n",
    "        ),\n",
    "        batch_size=None\n",
    "    )\n",
    "    torch_model.cuda()\n",
    "    model_prepared.cuda()\n",
    "    for data in tqdm(dataloader):\n",
    "        vals = model_prepared(*data)\n",
    "        if not isinstance(vals, (tuple, list)):\n",
    "            vals = (vals,)\n",
    "        vals = {name: val for name, val in zip(model_prepared.onnx_mapping.outputs, vals)}\n",
    "        optimizer.zero_grad()\n",
    "        loss_fn(vals, torch_model, data)\n",
    "        optimizer.step()\n",
    "    \n",
    "    model_prepared.cpu()\n",
    "    convert_custom_config = ConvertCustomConfig()\n",
    "    convert_custom_config.set_preserved_attributes(['onnx_mapping'])\n",
    "    model_converted = convert_fx(model_prepared, convert_custom_config=convert_custom_config)\n",
    "    fix_onnx_mapping(model_converted)\n",
    "    return model_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c2a3fc-58a3-44ed-bae6-0365b8112cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycgao/Workdir/psurvey/tools/onnxutils/onnxutils/onnx2torch/resize.py:45: UserWarning: `nn.functional.upsample_bilinear` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  x = nn.functional.upsample_bilinear(x, self.sizes)\n",
      "/opt/miniconda3/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "none_qconfig = QConfig(\n",
    "    activation=NoopObserver,\n",
    "    weight=NoopObserver\n",
    ")\n",
    "default_qconfig = QConfig(\n",
    "    activation=ReuseInputObserver,\n",
    "    weight=NoopObserver\n",
    ")\n",
    "conv2d_qconfig = QConfig(\n",
    "    activation=HistogramObserver.with_args(\n",
    "        reduce_range=True),\n",
    "    # activation=MinMaxObserver.with_args(\n",
    "    #     dtype=torch.qint8, qscheme=torch.per_tensor_symmetric),\n",
    "    weight=MinMaxObserver.with_args(\n",
    "        dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)\n",
    ")\n",
    "qconfig_mapping = (QConfigMapping()\n",
    "                       .set_global(none_qconfig)\n",
    "                       .set_module_name(normalize_module_name('/RoutingMaskHead/up_head/up_head.1/conv1/conv/Conv'), conv2d_qconfig)\n",
    "                       .set_module_name(normalize_module_name('/RoutingMaskHead/up_head/up_head.1/conv1/act/Relu'), conv2d_qconfig)\n",
    "                       )\n",
    "\n",
    "model_converted = quantize_model(torch_model, qconfig_mapping, DatasetUtils.take_front(dataset, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084662e0-7863-4490-b36f-baf04561e795",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17d47b6-d240-4fd9-a317-f0b12b4db4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxutils.quantization.metric import compute_metrics, print_stats\n",
    "\n",
    "def model_infer(model, data):\n",
    "    with torch.no_grad():\n",
    "        vals = model(*data)\n",
    "    if not isinstance(vals, (tuple, list)):\n",
    "        vals = (vals,)\n",
    "    return {name: val for name, val in zip(model.onnx_mapping.outputs, vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d15a27-d2d1-4be3-a72f-6f932faa2269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycgao/Workdir/psurvey/tools/onnxutils/onnxutils/onnx2torch/resize.py:45: UserWarning: `nn.functional.upsample_bilinear` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  x = nn.functional.upsample_bilinear(x, self.sizes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refline_instance_impassable_lane_mask {'snr': [0.0006998606259003282], 'mse': [7.35356380232588e-08], 'cosine': [0.9996775388717651]}\n",
      "refline_instance_passable_road_mask {'snr': [3.551760528353043e-05], 'mse': [1.516431666459539e-06], 'cosine': [0.9999885559082031]}\n",
      "refline_instance_passable_lane_mask {'snr': [2.8107297112001106e-05], 'mse': [2.3454799702449236e-06], 'cosine': [0.9999882578849792]}\n",
      "refline_instance_nearest_passable_lane_mask {'snr': [2.023341403400991e-05], 'mse': [1.521337793519706e-07], 'cosine': [0.9999995231628418]}\n",
      "feats_0 {'snr': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cosine': [1.0000076293945312, 1.0000081062316895, 1.000004768371582, 1.000006914138794, 1.0000076293945312, 1.0000059604644775, 1.0000076293945312, 1.000040888786316, 1.000005841255188, 1.0000065565109253]}\n",
      "feats_2 {'snr': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cosine': [1.0000011920928955, 1.0000014305114746, 1.0000009536743164, 1.000001072883606, 1.0000016689300537, 1.0000016689300537, 1.0000019073486328, 1.0000027418136597, 1.0000005960464478, 1.0000014305114746]}\n",
      "feats_1 {'snr': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cosine': [1.000002384185791, 1.000003457069397, 1.0000040531158447, 1.000004529953003, 1.000003457069397, 1.0000046491622925, 1.0000033378601074, 0.9999977350234985, 1.0000039339065552, 1.000004529953003]}\n",
      "tl_flatten_objectness {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.0000022649765015, 1.0000003576278687, 1.0000109672546387]}\n",
      "tl_flatten_bboxes {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.0000011920928955, 1.000002145767212, 1.0]}\n",
      "allsign_flatten_cls_scores {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.000003457069397, 1.0000039339065552, 1.000003695487976]}\n",
      "allsign_flatten_bboxes {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [0.9999998211860657, 1.0000001192092896, 1.0]}\n",
      "allsign_flatten_objectness {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.0000001192092896, 1.0000001192092896, 1.000000238418579]}\n",
      "tl_flatten_cls_scores {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.0000171661376953, 1.0000193119049072, 1.000016212463379]}\n",
      "mpp_pred_r {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [0.0, 0.0]}\n",
      "flatten_bindepths {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [0.9999998807907104, 1.000000238418579]}\n",
      "flatten_istls {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [1.000000238418579, 0.9999999403953552]}\n",
      "mpp_pred_t {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [0.0, 0.0]}\n",
      "refline_instance_stopline_uncertainty {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999998211860657]}\n",
      "smtc_offset_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000016689300537]}\n",
      "det3d_bbox_score {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000000238418579]}\n",
      "ego_velocity {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "use_passable_scenario {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "gct_sigma_dt {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "mono_results {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000001311302185]}\n",
      "refline_instance_stopline_keypoints {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "lane_mask_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000101327896118]}\n",
      "refline_instance_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "smtc_confidence_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000176429748535]}\n",
      "smtc_confidence_4 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000032186508179]}\n",
      "marker_class_softmax {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000001192092896]}\n",
      "refline_instance_close_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "smtc_class_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000023603439331]}\n",
      "refline_instance_keypoint {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000078797340393]}\n",
      "det3d_bboxes_category {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "smtc_offset_3 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000004768371582]}\n",
      "mono3d_fpn_3 {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999998211860657]}\n",
      "mono3d_fpn_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000001192092896]}\n",
      "det3d_bboxes_attr_scores {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000003576278687]}\n",
      "roadmask_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999979138374329]}\n",
      "volumes {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999995231628418]}\n",
      "bev2d_semantic_cls_result {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999995231628418]}\n",
      "weighted_cls_scores_flat {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000001311302185]}\n",
      "refline_instance_stopline_cls_score {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000001192092896]}\n",
      "det3d_decoded_bboxes {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000065565109253]}\n",
      "roadmask_dir_out {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000009536743164]}\n",
      "mpp_pose_fuse_state {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "roadmask_id {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000362396240234]}\n",
      "smtc_offset_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "det3d_bboxes_subcategory {'snr': [0.0], 'mse': [0.0], 'cosine': [1.00001859664917]}\n",
      "bev2d_cls_result {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000002384185791]}\n",
      "marker_confidence_out {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "refline_instance_seg {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000019073486328]}\n",
      "smtc_confidence_3 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000078678131104]}\n",
      "smtc_confidence_8 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000011920928955]}\n",
      "marker_class_uncertainty {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999995231628418]}\n",
      "smtc_confidence_7 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000141859054565]}\n",
      "smtc_class_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000157356262207]}\n",
      "smtc_confidence_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000114440917969]}\n",
      "mono3d_fpn_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000001192092896]}\n",
      "smtc_confidence_6 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000418424606323]}\n",
      "refline_instance_stopline_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "mono3d_fpn_0 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000025033950806]}\n",
      "refline_instance_impassable_lane_mask {'snr': [0.0012448971392586827], 'mse': [2.6324538993094393e-08], 'cosine': [0.9996061325073242]}\n",
      "refline_instance_nearest_passable_lane_mask {'snr': [3.7045487260911614e-05], 'mse': [2.842951971615548e-07], 'cosine': [0.9999896883964539]}\n",
      "refline_instance_passable_lane_mask {'snr': [3.638855923782103e-05], 'mse': [2.4947989913925994e-06], 'cosine': [0.9999839067459106]}\n",
      "refline_instance_passable_road_mask {'snr': [2.1447895051096566e-05], 'mse': [8.621337883596425e-07], 'cosine': [0.9999925494194031]}\n",
      "feats_0 {'snr': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cosine': [1.0000059604644775, 1.0000081062316895, 1.0000059604644775, 1.0000089406967163, 1.000008463859558, 1.000009536743164, 1.0000087022781372, 1.000040888786316, 1.000005841255188, 1.0000079870224]}\n",
      "feats_2 {'snr': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cosine': [1.000001072883606, 1.0000014305114746, 0.9999998211860657, 1.0000003576278687, 1.0000020265579224, 1.0000008344650269, 1.0000009536743164, 1.0000027418136597, 1.0000014305114746, 1.0000017881393433]}\n",
      "feats_1 {'snr': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cosine': [1.0000029802322388, 1.0000020265579224, 1.0000038146972656, 1.000004768371582, 1.0000026226043701, 1.0000035762786865, 1.0000041723251343, 0.9999977350234985, 1.0000030994415283, 1.0000029802322388]}\n",
      "tl_flatten_objectness {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.000004768371582, 1.0000007152557373, 1.0000180006027222]}\n",
      "tl_flatten_bboxes {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.0000011920928955, 1.000000238418579, 1.0000005960464478]}\n",
      "allsign_flatten_cls_scores {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.0000033378601074, 1.0000040531158447, 1.0000038146972656]}\n",
      "allsign_flatten_bboxes {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.0000009536743164, 0.9999997615814209, 1.0]}\n",
      "allsign_flatten_objectness {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [0.9999999403953552, 1.000000238418579, 1.0000003576278687]}\n",
      "tl_flatten_cls_scores {'snr': [0.0, 0.0, 0.0], 'mse': [0.0, 0.0, 0.0], 'cosine': [1.000016450881958, 1.0000183582305908, 1.0000154972076416]}\n",
      "mpp_pred_r {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [0.0, 0.0]}\n",
      "flatten_bindepths {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [0.9999998807907104, 0.9999998211860657]}\n",
      "flatten_istls {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [1.0000001192092896, 1.0]}\n",
      "mpp_pred_t {'snr': [0.0, 0.0], 'mse': [0.0, 0.0], 'cosine': [0.0, 0.0]}\n",
      "refline_instance_stopline_uncertainty {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "smtc_offset_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "det3d_bbox_score {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "ego_velocity {'snr': [0.0], 'mse': [0.0], 'cosine': [0.0]}\n",
      "use_passable_scenario {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "gct_sigma_dt {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999996423721313]}\n",
      "mono_results {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000001311302185]}\n",
      "refline_instance_stopline_keypoints {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "lane_mask_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000121593475342]}\n",
      "refline_instance_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999998807907104]}\n",
      "smtc_confidence_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000214576721191]}\n",
      "smtc_confidence_4 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000003457069397]}\n",
      "marker_class_softmax {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000001192092896]}\n",
      "refline_instance_close_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "smtc_class_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000253915786743]}\n",
      "refline_instance_keypoint {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000885725021362]}\n",
      "det3d_bboxes_category {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "smtc_offset_3 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000022649765015]}\n",
      "mono3d_fpn_3 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "mono3d_fpn_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000003576278687]}\n",
      "det3d_bboxes_attr_scores {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000017881393433]}\n",
      "roadmask_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999949932098389]}\n",
      "volumes {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000011920928955]}\n",
      "bev2d_semantic_cls_result {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "weighted_cls_scores_flat {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000001192092896]}\n",
      "refline_instance_stopline_cls_score {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "det3d_decoded_bboxes {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000009536743164]}\n",
      "roadmask_dir_out {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000014305114746]}\n",
      "mpp_pose_fuse_state {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999999403953552]}\n",
      "roadmask_id {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000373125076294]}\n",
      "smtc_offset_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000008344650269]}\n",
      "det3d_bboxes_subcategory {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000159740447998]}\n",
      "bev2d_cls_result {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000029802322388]}\n",
      "marker_confidence_out {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000001192092896]}\n",
      "refline_instance_seg {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000014305114746]}\n",
      "smtc_confidence_3 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000113248825073]}\n",
      "smtc_confidence_8 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.00001060962677]}\n",
      "marker_class_uncertainty {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "smtc_confidence_7 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000133514404297]}\n",
      "smtc_class_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000128746032715]}\n",
      "smtc_confidence_1 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000011682510376]}\n",
      "mono3d_fpn_2 {'snr': [0.0], 'mse': [0.0], 'cosine': [0.9999997019767761]}\n",
      "smtc_confidence_6 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0000393390655518]}\n",
      "refline_instance_stopline_confidence {'snr': [0.0], 'mse': [0.0], 'cosine': [1.0]}\n",
      "mono3d_fpn_0 {'snr': [0.0], 'mse': [0.0], 'cosine': [1.000002384185791]}\n"
     ]
    }
   ],
   "source": [
    "model_converted.cpu()\n",
    "torch_model.cpu()\n",
    "dataloader = DataLoader(\n",
    "    DatasetUtils.take_front(\n",
    "        DatasetUtils.transform(\n",
    "            dataset,\n",
    "            lambda item: tuple(item[x].cpu() for x in torch_model.onnx_mapping.inputs)\n",
    "        ),\n",
    "        2),\n",
    "    batch_size=None\n",
    ")\n",
    "\n",
    "for data in dataloader:\n",
    "    real = model_infer(torch_model, data)\n",
    "    pred = model_infer(model_converted, data)\n",
    "    analysis_reports = [compute_metrics(real, pred, metrics=['snr', 'mse', 'cosine'])]\n",
    "    print_stats(analysis_reports, sorted_metric='snr', reversed_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf70d2-9092-432e-b5ec-eef2035fb4c7",
   "metadata": {},
   "source": [
    "# Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d830438d-8397-4ef9-9349-286678a3c0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycgao/Workdir/psurvey/tools/onnxutils/onnxutils/onnx2torch/scatter_nd.py:38: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  output_indices = indices.reshape((-1, indices.shape[-1])).T.tolist()\n",
      "/opt/miniconda3/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:1959: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n",
      "/opt/miniconda3/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/opt/miniconda3/lib/python3.10/site-packages/torch/onnx/utils.py:663: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/opt/miniconda3/lib/python3.10/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "model_converted.cpu()\n",
    "torch.onnx.export(\n",
    "    model_converted,\n",
    "    dataset[0],\n",
    "    qmodel_path,\n",
    "    input_names=torch_model.onnx_mapping.inputs,\n",
    "    output_names=torch_model.onnx_mapping.outputs,\n",
    "    keep_initializers_as_inputs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8db84-3e20-47af-9230-1ad0edce1d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
